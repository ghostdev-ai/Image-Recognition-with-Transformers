{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOY95UvxlcIXd6mJEvq+/ce",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ugomezjr/Image-Recognition-with-Transformers/blob/main/pytorch_vision_transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "try:\n",
        "  from torchinfo import summary\n",
        "except:\n",
        "  print(\"[INFO] Couldn't find torchinfo... installing it.\")\n",
        "  !pip install -q torchinfo\n",
        "  from torchinfo import summary"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xkqBu-dcBKWW",
        "outputId": "7da90d35-27a7-43a8-85fc-4ae4b97d183d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Couldn't find torchinfo... installing it.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Equation 1: Patch + Position Embedding\n",
        "\n",
        "Split an image into fixed-sized patches, linearly embed each of them, add position embeddings.\n",
        "\n",
        "\\begin{aligned}\n",
        "\\mathbf{z}_{0} &=\\left[\\mathbf{x}_{\\text {class }} ; \\mathbf{x}_{p}^{1} \\mathbf{E} ; \\mathbf{x}_{p}^{2} \\mathbf{E} ; \\cdots ; \\mathbf{x}_{p}^{N} \\mathbf{E}\\right]+\\mathbf{E}_{\\text {pos }}, & & \\mathbf{E} \\in \\mathbb{R}^{\\left(P^{2} \\cdot C\\right) \\times D}, \\mathbf{E}_{\\text {pos }} \\in \\mathbb{R}^{(N+1) \\times D}\n",
        "\\end{aligned}"
      ],
      "metadata": {
        "id": "-cK0Ailb5jac"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RvrAgCrEmXlo"
      },
      "outputs": [],
      "source": [
        "# [0] Calculate patch embedding input and output shapes\n",
        "\n",
        "# Create variables to mimic terms\n",
        "height = 224 # H (height)\n",
        "width = 224 # W (width)\n",
        "color_channels = 3 # C (color channels)\n",
        "patch_size = 16 # P (image patch resolution)\n",
        "\n",
        "# N = HW/P^2 (number of patches)\n",
        "number_of_patches = int((height * width) / patch_size**2)\n",
        "\n",
        "# Input shape (training resolution)\n",
        "embedding_layer_input_shape = (height, width, color_channels)\n",
        "\n",
        "# Output shape (sequence of flattened 2D patches), xp = N, (P^2 * C)\n",
        "embedding_layer_output_shape = (number_of_patches, patch_size**2 * color_channels) # (number of patches, embedding dimension)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Number of Patches: {number_of_patches}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bssLuZ_J7NEm",
        "outputId": "a8f752fd-802a-4af3-b9ad-774839d3b9bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Patches: 196\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Input shape: {embedding_layer_input_shape}\\nOutput shape: {embedding_layer_output_shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uwnqn7Df858t",
        "outputId": "89e8f94e-b18d-445c-b536-9071f6521d37"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input shape: (224, 224, 3)\n",
            "Output shape: (196, 768)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hybrid Architecture.\n",
        "\n",
        "The input sequence can be formed from feature maps of a CNN.  \n",
        "\n",
        "Patch embedding projection E is applied to patches extracted from a CNN feature map. \n",
        "\n",
        "Input sequence is obtained by simply flattening the spatial dimensions of the feature map and projecting to the Transformer dimension."
      ],
      "metadata": {
        "id": "5LIKOsHvD64y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# [1] Split an image into fixed-sized patches\n",
        "\n",
        "image = torch.randn(3, 224, 224)\n",
        "\n",
        "# Create the Conv2d layer with hyperparameters from the ViT paper\n",
        "\n",
        "# Set \"kernel_size\" and \"stride\" equal to \"patch_size\" to effectivley get a layer that \n",
        "# splits our image into patches and creates a learnable embedding (referred to as a \"Linear Projection\" \n",
        "# in the ViT paper) of each patch.\n",
        "\n",
        "conv2d = nn.Conv2d(in_channels=color_channels, # number of color channels\n",
        "                   out_channels=(patch_size**2 * color_channels), # Hidden size D, this is the embedding size (768)\n",
        "                   kernel_size=patch_size,\n",
        "                   stride=patch_size,\n",
        "                   padding=0)"
      ],
      "metadata": {
        "id": "BnamhOHU9hRg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output = conv2d(image.unsqueeze(0)) # add a single batch dimension\n",
        "print(f\"Input shape (2D image): {image.shape}\\nOutput shape (flattened 2D Patches): {output.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cgCE2ibNHVx3",
        "outputId": "445cd2a3-8133-47f4-a396-70dd782810f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input shape (2D image): torch.Size([3, 224, 224])\n",
            "Output shape (flattened 2D Patches): torch.Size([1, 768, 14, 14])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output[0, 0, :, :]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AMvUM-pD9A_h",
        "outputId": "fb604cb9-d15e-430d-c3cc-3e240daaa310"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.2617,  0.5375,  0.5653,  0.0818, -0.7199,  1.2233,  0.9079, -1.1796,\n",
              "          1.3807,  0.7522,  0.1465, -0.1028, -0.4151, -0.9094],\n",
              "        [ 0.4685,  1.0137,  0.7614,  0.4743,  0.8700,  0.6178, -0.5635,  0.2802,\n",
              "         -0.5641,  0.2237, -0.1927,  0.8294, -0.3786,  0.1160],\n",
              "        [ 0.2053, -0.7453, -0.3989, -0.1740,  0.3718, -0.7562,  0.4569, -0.0475,\n",
              "          0.3184,  0.4324,  1.0421,  0.4952,  0.1185, -0.1388],\n",
              "        [ 0.3724, -0.3266, -1.2162,  0.5713,  0.0296, -0.0210,  0.9155, -0.9453,\n",
              "         -0.4576, -0.3443,  0.2580,  0.5521, -0.4516, -0.5196],\n",
              "        [ 0.0482, -0.0529, -0.3017, -0.3553, -0.4866, -0.3150,  0.0473,  0.2393,\n",
              "         -0.6070,  0.0756,  0.5224,  0.2691,  0.5255,  0.0338],\n",
              "        [ 0.1768,  0.2348, -0.2976, -0.1317, -0.7279,  0.3120,  0.8582,  0.7593,\n",
              "         -0.2002,  0.5486,  0.5005, -1.1663,  0.4659,  0.0623],\n",
              "        [-0.7629, -0.5654,  0.0653,  0.2391, -0.2418, -0.5778,  0.7394,  0.7111,\n",
              "          0.2660, -0.2707,  0.0930,  0.0960, -0.3198,  0.3457],\n",
              "        [-0.1577, -0.0911, -0.2590, -0.5233, -0.0598, -0.3885, -0.1811,  0.0679,\n",
              "          0.3799, -1.0670,  0.4174, -0.3841,  0.0130,  0.1292],\n",
              "        [ 0.1407, -0.1628, -0.1613,  0.0600, -0.1015,  0.3601,  0.5910, -0.2547,\n",
              "         -0.0297, -0.4368,  0.0399,  0.7890, -0.0323,  0.8147],\n",
              "        [ 1.3946, -0.0983,  0.5330,  1.3984, -0.4340,  0.5520, -0.7485, -0.5166,\n",
              "         -0.2960, -0.8065,  0.6596,  1.1266,  0.7086,  0.2459],\n",
              "        [ 0.2264, -0.3536, -0.3427,  0.2346,  0.0552, -0.2060,  1.2086, -0.6069,\n",
              "         -0.3040,  0.3581,  0.2702,  0.1578, -0.1953,  0.3256],\n",
              "        [ 0.7323,  0.0396, -0.1716, -0.2606,  0.1291,  0.6197, -0.5425,  1.5500,\n",
              "         -0.9090, -0.0462,  0.6880, -0.1080,  0.1422, -0.3695],\n",
              "        [-0.1405, -1.1040, -0.6792, -0.7961,  0.0425, -0.2647,  0.4219,  0.0051,\n",
              "          0.5756,  0.1864,  0.3322,  1.4224,  0.5365, -0.0861],\n",
              "        [ 0.0257,  0.5160, -0.1188,  1.3500, -0.5229, -0.3448,  0.1587,  1.2042,\n",
              "         -1.3207, -0.8928, -0.2933, -0.1441, -1.2477,  0.2955]],\n",
              "       grad_fn=<SliceBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Desired Output(1D sequence of flattened 2D patches):** (196, 768) -> (Number of Patches, Embedding Dimension) -> ${N \\times\\left(P^{2} \\cdot C\\right)}$"
      ],
      "metadata": {
        "id": "uXPf5XFryc2G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# [2] Flattening the patch embedding\n",
        "\n",
        "# Create flatten layer (14*14 == 196)\n",
        "flatten = nn.Flatten(start_dim=2, # flatten feature map height (dimension 2)\n",
        "                     end_dim=3) # flatten feature map width (dimension 3)\n",
        "\n",
        "# (number of patches, embedding dimension)\n",
        "print(f\"Output shape (1D sequence of flattened 2D patches): {flatten(output).permute(0, 2, 1).shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vNFihjB9OyIM",
        "outputId": "6aa3878e-d938-4a66-a75b-110c713e1c87"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output shape (1D sequence of flattened 2D patches): torch.Size([1, 196, 768])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "flatten(output)[0, 0, :]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qp21uarP9FMA",
        "outputId": "0c2847e0-4e0a-4ebd-dbe0-6580b56669d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([-0.2617,  0.5375,  0.5653,  0.0818, -0.7199,  1.2233,  0.9079, -1.1796,\n",
              "         1.3807,  0.7522,  0.1465, -0.1028, -0.4151, -0.9094,  0.4685,  1.0137,\n",
              "         0.7614,  0.4743,  0.8700,  0.6178, -0.5635,  0.2802, -0.5641,  0.2237,\n",
              "        -0.1927,  0.8294, -0.3786,  0.1160,  0.2053, -0.7453, -0.3989, -0.1740,\n",
              "         0.3718, -0.7562,  0.4569, -0.0475,  0.3184,  0.4324,  1.0421,  0.4952,\n",
              "         0.1185, -0.1388,  0.3724, -0.3266, -1.2162,  0.5713,  0.0296, -0.0210,\n",
              "         0.9155, -0.9453, -0.4576, -0.3443,  0.2580,  0.5521, -0.4516, -0.5196,\n",
              "         0.0482, -0.0529, -0.3017, -0.3553, -0.4866, -0.3150,  0.0473,  0.2393,\n",
              "        -0.6070,  0.0756,  0.5224,  0.2691,  0.5255,  0.0338,  0.1768,  0.2348,\n",
              "        -0.2976, -0.1317, -0.7279,  0.3120,  0.8582,  0.7593, -0.2002,  0.5486,\n",
              "         0.5005, -1.1663,  0.4659,  0.0623, -0.7629, -0.5654,  0.0653,  0.2391,\n",
              "        -0.2418, -0.5778,  0.7394,  0.7111,  0.2660, -0.2707,  0.0930,  0.0960,\n",
              "        -0.3198,  0.3457, -0.1577, -0.0911, -0.2590, -0.5233, -0.0598, -0.3885,\n",
              "        -0.1811,  0.0679,  0.3799, -1.0670,  0.4174, -0.3841,  0.0130,  0.1292,\n",
              "         0.1407, -0.1628, -0.1613,  0.0600, -0.1015,  0.3601,  0.5910, -0.2547,\n",
              "        -0.0297, -0.4368,  0.0399,  0.7890, -0.0323,  0.8147,  1.3946, -0.0983,\n",
              "         0.5330,  1.3984, -0.4340,  0.5520, -0.7485, -0.5166, -0.2960, -0.8065,\n",
              "         0.6596,  1.1266,  0.7086,  0.2459,  0.2264, -0.3536, -0.3427,  0.2346,\n",
              "         0.0552, -0.2060,  1.2086, -0.6069, -0.3040,  0.3581,  0.2702,  0.1578,\n",
              "        -0.1953,  0.3256,  0.7323,  0.0396, -0.1716, -0.2606,  0.1291,  0.6197,\n",
              "        -0.5425,  1.5500, -0.9090, -0.0462,  0.6880, -0.1080,  0.1422, -0.3695,\n",
              "        -0.1405, -1.1040, -0.6792, -0.7961,  0.0425, -0.2647,  0.4219,  0.0051,\n",
              "         0.5756,  0.1864,  0.3322,  1.4224,  0.5365, -0.0861,  0.0257,  0.5160,\n",
              "        -0.1188,  1.3500, -0.5229, -0.3448,  0.1587,  1.2042, -1.3207, -0.8928,\n",
              "        -0.2933, -0.1441, -1.2477,  0.2955], grad_fn=<SliceBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "flatten(output).permute(0, 2, 1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ekzpmMBc-MqO",
        "outputId": "b2000571-5106-4b71-9584-5a9a1b962c6a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[-0.2617, -0.9701,  0.1095,  ...,  0.0537,  1.0546,  0.1155],\n",
              "         [ 0.5375,  0.4571,  0.8763,  ...,  0.0760, -0.2131, -0.1085],\n",
              "         [ 0.5653, -0.5079,  0.6855,  ..., -0.1471,  0.3205, -0.5650],\n",
              "         ...,\n",
              "         [-0.1441, -1.3473,  0.1196,  ...,  1.0172,  0.4318,  0.8929],\n",
              "         [-1.2477, -0.9158, -0.7643,  ...,  1.1499, -0.5391,  0.2153],\n",
              "         [ 0.2955, -0.6296, -0.1625,  ..., -0.2562,  0.4158, -0.0259]]],\n",
              "       grad_fn=<PermuteBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Turn ViT patch embedding layer into a PyTorch module. \n",
        "\n",
        "1. Create a `PatchEmbedding` class and subclass `nn.Module`.\n",
        "2. Initialize class parameters, `in_channels=3`, `patch_size=16` and `embedding_dim=768`.\n",
        "3. Create `nn.Conv2d()` layer.\n",
        "4. Create `nn.Flatten()` layer to flatten the spatial dimensions of the feature map into a 1D learnable embedding.\n",
        "5. Define `forward()` method. \n",
        "6. Ensure the output shape is consistent with that of the ViT architecture (${N \\times\\left(P^{2} \\cdot C\\right)}$)\n"
      ],
      "metadata": {
        "id": "4RFKR1N_2A-s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PatchEmbedding(nn.Module):\n",
        "  \"\"\"Turns a 2D input image into a 1D sequence learnable embedding vector.\n",
        "\n",
        "  Args: \n",
        "    in_channels (int): Number of color channels for the input images. Defaults to 3.\n",
        "    patch_size (int): Size of patches to convert input image into. Defaults to 16.\n",
        "    embedding_dim (int): Size of embedding to turn image into. Defaults to 768.\n",
        "  \"\"\"\n",
        "  def __init__(self, \n",
        "               in_channels: int=3, \n",
        "               patch_size: int=16, \n",
        "               embedding_dim: int=768):\n",
        "    super().__init__()\n",
        "\n",
        "    self.conv = nn.Conv2d(in_channels=in_channels,\n",
        "                      out_channels=embedding_dim,\n",
        "                      kernel_size=patch_size,\n",
        "                      stride=patch_size,\n",
        "                      padding=0)\n",
        "                      \n",
        "    self.flatten = nn.Flatten(start_dim=2,\n",
        "                              end_dim=3)\n",
        "    \n",
        "  def forward(self, x):\n",
        "    return self.flatten(self.conv(x)).permute(0, 2, 1)"
      ],
      "metadata": {
        "id": "PIXOFN5ZwvfR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding = PatchEmbedding()\n",
        "embedding(image.unsqueeze(0)), embedding(image.unsqueeze(0)).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P0XsTAmI4xxN",
        "outputId": "2b5fad3b-150f-4379-c0ae-76cf1332cfa6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[[ 0.1902,  0.6967,  0.6295,  ...,  0.1795, -0.0255, -0.0537],\n",
              "          [ 0.0256,  0.0761, -0.5711,  ..., -1.0734, -0.2553, -0.9181],\n",
              "          [ 0.4979,  0.1233,  0.2194,  ..., -0.1984,  0.0070, -0.0337],\n",
              "          ...,\n",
              "          [-0.1950, -0.4613, -0.1825,  ..., -0.8430,  0.3619, -1.3511],\n",
              "          [ 0.4200, -0.1371,  0.7837,  ..., -0.3174,  0.5432, -0.5316],\n",
              "          [-0.2547,  0.0764,  0.3250,  ...,  1.0624,  0.2980,  0.7755]]],\n",
              "        grad_fn=<PermuteBackward0>), torch.Size([1, 196, 768]))"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get a summary of the input and outputs of PatchEmbedding\n",
        "summary(PatchEmbedding(),\n",
        "        input_size=(1, color_channels, height, width),\n",
        "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
        "        col_width=20,\n",
        "        row_settings=[\"var_names\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K0mn4hKhAouH",
        "outputId": "0e3a698a-9f8b-4012-9890-eef11787bed5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "========================================================================================================================\n",
              "Layer (type (var_name))                  Input Shape          Output Shape         Param #              Trainable\n",
              "========================================================================================================================\n",
              "PatchEmbedding (PatchEmbedding)          [1, 3, 224, 224]     [1, 196, 768]        --                   True\n",
              "├─Conv2d (conv)                          [1, 3, 224, 224]     [1, 768, 14, 14]     590,592              True\n",
              "├─Flatten (flatten)                      [1, 768, 14, 14]     [1, 768, 196]        --                   --\n",
              "========================================================================================================================\n",
              "Total params: 590,592\n",
              "Trainable params: 590,592\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (M): 115.76\n",
              "========================================================================================================================\n",
              "Input size (MB): 0.60\n",
              "Forward/backward pass size (MB): 1.20\n",
              "Params size (MB): 2.36\n",
              "Estimated Total Size (MB): 4.17\n",
              "========================================================================================================================"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# [3] Creating the class token embedding\n",
        "\n",
        "patch_embedded_image = embedding(image.unsqueeze(0))\n",
        "\n",
        "# Get batch size and embedding dimension\n",
        "batch_size = patch_embedded_image.shape[0]\n",
        "embedding_dim = patch_embedded_image.shape[2] # D (embedding dimension)\n",
        "\n",
        "# Create the class token embedding as a learnable parameter that shares the same size as the embedding dimension (D)\n",
        "class_token = nn.Parameter(torch.ones(batch_size, 1, embedding_dim), # [batch size, number of tokens, embedding dimension]\n",
        "                           requires_grad=True) # ensure embedding is learnable\n",
        "\n",
        "print(f\"{class_token.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KxMVd_9wBN6H",
        "outputId": "bbb15aa6-03b1-40fa-ff4a-573cd55d1e8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 1, 768])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# [4] Prepend a learnable embedding to the sequence of embedded patches\n",
        "patch_embedding_with_class_token = torch.cat((class_token, patch_embedded_image), \n",
        "                                             dim=1)\n",
        "\n",
        "# (batch size, class token + number of patches, embedding dimension)\n",
        "print(patch_embedding_with_class_token, patch_embedding_with_class_token.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tajul07MOZk8",
        "outputId": "1e9cb92e-994f-47ff-e753-fc32189a4794"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
            "         [ 0.1902,  0.6967,  0.6295,  ...,  0.1795, -0.0255, -0.0537],\n",
            "         [ 0.0256,  0.0761, -0.5711,  ..., -1.0734, -0.2553, -0.9181],\n",
            "         ...,\n",
            "         [-0.1950, -0.4613, -0.1825,  ..., -0.8430,  0.3619, -1.3511],\n",
            "         [ 0.4200, -0.1371,  0.7837,  ..., -0.3174,  0.5432, -0.5316],\n",
            "         [-0.2547,  0.0764,  0.3250,  ...,  1.0624,  0.2980,  0.7755]]],\n",
            "       grad_fn=<CatBackward0>) torch.Size([1, 197, 768])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Position Embedding Shape ($\\mathbf{E}_{\\text {pos }}$):\n",
        "\n",
        "\\begin{aligned}\n",
        "\\mathbf{E}_{\\text {pos }} \\in \\mathbb{R}^{(N+1) \\times D}\n",
        "\\end{aligned}\n",
        "\n",
        "Where:\n",
        "\n",
        "*   $N=H W / P^{2}$ is the number of patches.\n",
        "*   $D$ is the **patch embedding** dimension.\n",
        "\n"
      ],
      "metadata": {
        "id": "mAZHma-sRwva"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# [5] Create the position embedding\n",
        "\n",
        "# Create the learnable 1D position embedding\n",
        "position_embedding = nn.Parameter(torch.ones(batch_size, \n",
        "                                             number_of_patches + 1, \n",
        "                                             embedding_dim),\n",
        "                                  requires_grad=True)\n",
        "\n",
        "# (batch_size, number of patches, embedding dimension)\n",
        "print(f\"Position Embedding shape: {position_embedding.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "atd8812hQRzj",
        "outputId": "7a6883d9-31d9-4166-b3ff-8dad3e2aed84"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Position Embedding shape: torch.Size([1, 197, 768])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# [6] Add position embeddings to the patch embeddings to retain positional information\n",
        "\n",
        "# Add the position embedding to the class token and patch embedding\n",
        "patch_and_position_embedding = patch_embedding_with_class_token + position_embedding\n",
        "\n",
        "print(patch_and_position_embedding)\n",
        "print(f\"Patch + Position Embedding shape: {patch_and_position_embedding.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qFCfnOD1VEoK",
        "outputId": "c0bcb5a8-22c3-4e0b-b24c-09390bd293af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[ 2.0000,  2.0000,  2.0000,  ...,  2.0000,  2.0000,  2.0000],\n",
            "         [ 1.1902,  1.6967,  1.6295,  ...,  1.1795,  0.9745,  0.9463],\n",
            "         [ 1.0256,  1.0761,  0.4289,  ..., -0.0734,  0.7447,  0.0819],\n",
            "         ...,\n",
            "         [ 0.8050,  0.5387,  0.8175,  ...,  0.1570,  1.3619, -0.3511],\n",
            "         [ 1.4200,  0.8629,  1.7837,  ...,  0.6826,  1.5432,  0.4684],\n",
            "         [ 0.7453,  1.0764,  1.3250,  ...,  2.0624,  1.2980,  1.7755]]],\n",
            "       grad_fn=<AddBackward0>)\n",
            "Patch + Position Embedding shape: torch.Size([1, 197, 768])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### From Image to Patch + Position Embedding (Extra learnable [class] embedding)\n",
        "\n",
        "\\begin{aligned}\n",
        "\\mathbf{z}_{0} &=\\left[\\mathbf{x}_{\\text {class }} ; \\mathbf{x}_{p}^{1} \\mathbf{E} ; \\mathbf{x}_{p}^{2} \\mathbf{E} ; \\cdots ; \\mathbf{x}_{p}^{N} \\mathbf{E}\\right]+\\mathbf{E}_{\\text {pos }}, & & \\mathbf{E} \\in \\mathbb{R}^{\\left(P^{2} \\cdot C\\right) \\times D}, \\mathbf{E}_{\\text {pos }} \\in \\mathbb{R}^{(N+1) \\times D}\n",
        "\\end{aligned}\n",
        "\n",
        "1. Set the patch size.\n",
        "2. Get a single image, print its shape and store it's height and width.\n",
        "3. Add batch dimension to the single image for our `PatchEmbedding` layer.\n",
        "4. Create a `PatchEmbedding` layer with a `patch_size=16` and `embedding_dim=768`.\n",
        "5. Pass the single image through `PatchEmbedding` layer to create a sequence of patch embeddings. "
      ],
      "metadata": {
        "id": "tcqLBRuYWWXj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "torch.cuda.manual_seed(42)\n",
        "\n",
        "patch_size = 16\n",
        "\n",
        "print(f\"Image shape: {image.shape}\")\n",
        "\n",
        "height = image.shape[1]\n",
        "width = image.shape[2]\n",
        "\n",
        "patch_embedding = PatchEmbedding() # patch_size=16 and embedding_dim=768 by default. \n",
        "\n",
        "embedded_patches = patch_embedding(image.unsqueeze(0)) # add batch dimension\n",
        "\n",
        "print(f\"Embedded Patches shape: {embedded_patches.shape}\")\n",
        "\n",
        "class_tokens = nn.Parameter(torch.ones(batch_size, 1, embedding_dim), \n",
        "                            requires_grad=True)\n",
        "\n",
        "embedded_patches_with_tokens = torch.cat((class_tokens, embedded_patches), \n",
        "                             dim=1)\n",
        "\n",
        "print(f\"Embedded Patches with extra learnable [class] embedding shape: {embedded_patches_with_tokens.shape}\")\n",
        "\n",
        "position_embedding = nn.Parameter(torch.ones(batch_size, number_of_patches + 1, embedding_dim), \n",
        "                                  requires_grad=True)\n",
        "\n",
        "patch_and_position_embedding_with_tokens = embedded_patches_with_tokens + position_embedding\n",
        "\n",
        "print(f\"Patch + Position Embedding shape: {patch_and_position_embedding_with_tokens.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a6ig2P4KXywM",
        "outputId": "102cbff4-da87-41b3-bd52-a2f3122cae1a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image shape: torch.Size([3, 224, 224])\n",
            "Embedded Patches shape: torch.Size([1, 196, 768])\n",
            "Embedded Patches with extra learnable [class] embedding shape: torch.Size([1, 197, 768])\n",
            "Patch + Position Embedding shape: torch.Size([1, 197, 768])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Equation 2: Multi-Head Attention (MSA)\n",
        "\n",
        "A Multi-Head Attention (MSA) layer wrapped in a LayerNorm (LN) layer with a residual connection.\n",
        "\n",
        "\\begin{aligned}\n",
        "\\mathbf{z}_{\\ell}^{\\prime} &=\\operatorname{MSA}\\left(\\operatorname{LN}\\left(\\mathbf{z}_{\\ell-1}\\right)\\right)+\\mathbf{z}_{\\ell-1}, & & \\ell=1 \\ldots L\n",
        "\\end{aligned}\n",
        "\n",
        "Where:\n",
        "\n",
        "* `Layer Normalization` (LayerNorm) normalizes an input over the last dimension.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "XE8gC2GQVRGV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Replicate Multi-Head Attension (MSA) with PyTorch layers\n",
        "\n",
        "1. Create a class called `MultiheadSelfAttentionBlock` that inherits from `torch.nn.Module`.\n",
        "2. Initialize the class with hyperparameters from Table 1 of the ViT paper for the ViT-Base model.\n",
        "3. Create a layer normalization (LN) layer with `torch.nn.LayerNorm()` with the `normalized_shape` parameter the same as our embedding dimension ($D$ from Table 1).\n",
        "4. Create a multi-head attention (MSA) layer with the appropriate `embed_dim`, `num_heads`, `dropout` and `batch_first` parameters.\n",
        "5. Create a `forward()` method for our class passing the inputs through the LN layer and MSA layer."
      ],
      "metadata": {
        "id": "PpkYmwxkala7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadSelfAttention(nn.Module):\n",
        "  def __init__(self, \n",
        "               embedding_dim: int=768,\n",
        "               num_heads: int=12,\n",
        "               attn_dropout: float=0):\n",
        "    super().__init__()\n",
        "\n",
        "    self.layer_norm = nn.LayerNorm(normalized_shape=768)\n",
        "     \n",
        "    self.multihead_attn = nn.MultiheadAttention(embed_dim=embedding_dim,\n",
        "                                           num_heads=num_heads,\n",
        "                                           dropout=attn_dropout,\n",
        "                                           batch_first=True)\n",
        "    \n",
        "  def forward(self, xp):\n",
        "    xp = self.layer_norm(xp)\n",
        "    attn_out, _ = self.multihead_attn(query=xp, \n",
        "                                      key=xp, \n",
        "                                      value=xp,\n",
        "                                      need_weights=False)\n",
        "    return attn_out\n",
        "\n"
      ],
      "metadata": {
        "id": "xGsEWqVkVwFS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize an instance of MSABlock\n",
        "attention = MultiHeadSelfAttention()\n",
        "\n",
        "# Forward pass patch + position embedding through MSABlock\n",
        "print(f\"Input shape (Embedded Patches): {patch_and_position_embedding_with_tokens.shape}\")\n",
        "print(f\"Output shape (MSA Block): {attention(patch_and_position_embedding_with_tokens).shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "InRIKFK6fhct",
        "outputId": "8674ec6b-c6e1-458f-f7f6-36c72fde8a38"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input shape (Embedded Patches): torch.Size([1, 197, 768])\n",
            "Output shape (MSA Block): torch.Size([1, 197, 768])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Equation 3: Multilayer Perceptron (MLP)\n",
        "\n",
        "\\begin{aligned}\n",
        "\\mathbf{z}_{\\ell} &=\\operatorname{MLP}\\left(\\operatorname{LN}\\left(\\mathbf{z}_{\\ell}^{\\prime}\\right)\\right)+\\mathbf{z}_{\\ell}^{\\prime}, & & \\ell=1 \\ldots L \\\\\n",
        "\\end{aligned}\n",
        "\n",
        "where:\n",
        "\n",
        "* `Multilayer Perceptron` (MLP) contains two linear layers with a GELU (Gaussian Error Linear Units) non-linearity and Dropout layers.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Q8qBTyVzYwia"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Replicating Multilayer Perceptron (MLP) with PyTorch layers\n",
        "\n",
        "1. Create a class called `MLPBlock` that inherits from `torch.nn.Module`.\n",
        "2. Initialize the class with hyperparameters from Table 1 and Table 3 of the ViT paper for the ViT-Base model. \n",
        "3. Create a layer normalization (LN) layer with `torch.nn.LayerNorm()` with the `normalized_shape` parameter the same as our embedding dimension ($D$ from Table 1).\n",
        "4. Create a sequential series of MLP layer(s) using `torch.nn.Linear()`, `torch.nn.Dropout()`, and `torch.nn.GELU()` with appropriate hyperparameter values from Table 1 and Table 3.\n",
        "5. Create a `forward()` method for our class passing in the inputs through the LN layer and MLP layer(s)."
      ],
      "metadata": {
        "id": "xfGkPkaugB-7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultilayerPerceptron(nn.Module):\n",
        "  def __init__(self, \n",
        "               embedding_dim: int=768, \n",
        "               mlp_size: int=3072, \n",
        "               dropout: float=0.1):\n",
        "    super().__init__()\n",
        "\n",
        "    self.layer_norm = nn.LayerNorm(normalized_shape=embedding_dim)\n",
        "\n",
        "    self.mlp = nn.Sequential(\n",
        "        nn.Linear(in_features=embedding_dim,\n",
        "                  out_features=mlp_size),\n",
        "        nn.GELU(),\n",
        "        nn.Dropout(p=dropout, \n",
        "                   inplace=True),\n",
        "        nn.Linear(in_features=mlp_size,\n",
        "                  out_features=embedding_dim),\n",
        "        nn.Dropout(p=dropout,\n",
        "                   inplace=True)\n",
        "    )\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.mlp(self.layer_norm(x))"
      ],
      "metadata": {
        "id": "08HCd64VgaTh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mlp = MultilayerPerceptron()\n",
        "\n",
        "print(f\"Input shape (MSA Block): {attention(patch_and_position_embedding_with_tokens).shape}\")\n",
        "print(f\"Output shape (MLP Block): {mlp(attention(patch_and_position_embedding_with_tokens)).shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "21EbmalbqSr3",
        "outputId": "210078ad-18f0-4322-b84a-96fe9e10cd46"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input shape (MSA Block): torch.Size([1, 197, 768])\n",
            "Output shape (MLP Block): torch.Size([1, 197, 768])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup Transformer Encoder\n",
        "\n",
        "`Encoder` or `Auto Encoder` refers to a stack of layers that \"encodes\" an input (turns into some numerical representation).\n",
        "\n",
        "The Transformer Encoder will encode our patched image embedding into a learned representation consisting of alternating layers of MSA blocks and MLP blocks. Layernorm (LN) is applied before every block, and residual connections after every block.\n",
        "\n",
        "One of the main ideas beding residual connections is that they prevent weight values and gradient updates from getting too small and thus allow deeper networks and in turn allow deeper representations to be learned. \n",
        "\n"
      ],
      "metadata": {
        "id": "tTKaXcicvppO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Replicate Transformer Encoder with MSA and MLP blocks\n",
        "\n",
        "1. Create a class called `TransformerEncoderBlock` that inherits from `torch.nn.Module`.\n",
        "2. Initialize the class with hyperparameters from Table 1 and Table 3 of the ViT paper for the ViT-Base model. \n",
        "3. Instantiate a MSA block for equation 2 using our `MultiheadSelfAttentionBlock` with the appropriate parameters. \n",
        "4. Instantiate a MLP block for equation 3 using our `MLPBlock` with the appropriate parameters. \n",
        "5. Create a `forward()` method for our `TransformerEncoderBlock` class.\n",
        "6. Create a residual connection for the MSA block (for equation 2).\n",
        "7. Create a residual connection for the MLP block (for equation 3)."
      ],
      "metadata": {
        "id": "bCg2PwVWy7PN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerEncoder(nn.Module):\n",
        "  def __init__(self, \n",
        "               embedding_dim: int=768,\n",
        "               mlp_size: int=3072,\n",
        "               num_heads: int=12,\n",
        "               attn_dropout: float=0.0,\n",
        "               mlp_dropout: float=0.1):\n",
        "    super().__init__()\n",
        "    \n",
        "    self.multihead_attn = MultiHeadSelfAttention(embedding_dim=embedding_dim, \n",
        "                                                 num_heads=num_heads,\n",
        "                                                 attn_dropout=attn_dropout)\n",
        "    \n",
        "    self.mlp = MultilayerPerceptron(embedding_dim=embedding_dim,\n",
        "                                    mlp_size=mlp_size,\n",
        "                                    dropout=mlp_dropout)\n",
        "    \n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.multihead_attn(x) + x\n",
        "    x = self.mlp(x) + x\n",
        "    return x"
      ],
      "metadata": {
        "id": "TlXy7R_CwP0w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = TransformerEncoder()\n",
        "\n",
        "print(f\"Input shape (Embedded Patches): {patch_and_position_embedding_with_tokens.shape}\")\n",
        "print(f\"Output shape (Transformer Encoder): {encoder(patch_and_position_embedding_with_tokens).shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NwoZwNiS2yyv",
        "outputId": "d5fa9eb8-7dc8-4dfc-e04b-3d17731d2eaf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input shape (Embedded Patches): torch.Size([1, 197, 768])\n",
            "Output shape (Transformer Encoder): torch.Size([1, 197, 768])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "summary(model=encoder,\n",
        "        input_size=(1, 197, 768),\n",
        "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
        "        col_width=20,\n",
        "        row_settings=[\"var_names\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bmvefnzZ4dyg",
        "outputId": "ec3220ff-29a1-419c-ace0-fda807630942"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "==================================================================================================================================\n",
              "Layer (type (var_name))                            Input Shape          Output Shape         Param #              Trainable\n",
              "==================================================================================================================================\n",
              "TransformerEncoder (TransformerEncoder)            [1, 197, 768]        [1, 197, 768]        --                   True\n",
              "├─MultiHeadSelfAttention (multihead_attn)          [1, 197, 768]        [1, 197, 768]        --                   True\n",
              "│    └─LayerNorm (layer_norm)                      [1, 197, 768]        [1, 197, 768]        1,536                True\n",
              "│    └─MultiheadAttention (multihead_attn)         --                   [1, 197, 768]        2,362,368            True\n",
              "├─MultilayerPerceptron (mlp)                       [1, 197, 768]        [1, 197, 768]        --                   True\n",
              "│    └─LayerNorm (layer_norm)                      [1, 197, 768]        [1, 197, 768]        1,536                True\n",
              "│    └─Sequential (mlp)                            [1, 197, 768]        [1, 197, 768]        --                   True\n",
              "│    │    └─Linear (0)                             [1, 197, 768]        [1, 197, 3072]       2,362,368            True\n",
              "│    │    └─GELU (1)                               [1, 197, 3072]       [1, 197, 3072]       --                   --\n",
              "│    │    └─Dropout (2)                            [1, 197, 3072]       [1, 197, 3072]       --                   --\n",
              "│    │    └─Linear (3)                             [1, 197, 3072]       [1, 197, 768]        2,360,064            True\n",
              "│    │    └─Dropout (4)                            [1, 197, 768]        [1, 197, 768]        --                   --\n",
              "==================================================================================================================================\n",
              "Total params: 7,087,872\n",
              "Trainable params: 7,087,872\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (M): 4.73\n",
              "==================================================================================================================================\n",
              "Input size (MB): 0.61\n",
              "Forward/backward pass size (MB): 8.47\n",
              "Params size (MB): 18.90\n",
              "Estimated Total Size (MB): 27.98\n",
              "=================================================================================================================================="
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup Transformer Encoder with PyTorch's `torch.nn.TransformerEncoderLayer()`\n",
        "\n",
        "The ViT-Base architecture uses 12 `torch.nn.TransformerEncoderLayer()` stacked on top of each, this can be with `torch.nn.TransformerEncoder(encoder_layer, num_layers)` where:\n",
        "\n",
        "* `encoder_layer` - The target Transformer Encoder layer created with `torch.nn.TransformerEncoderLayer()`.\n",
        "* `num_layers` - The number of Transformer Encoder layers to stack together. "
      ],
      "metadata": {
        "id": "NKpc2pYP5FNp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn.functional import gelu\n",
        "encoder_pytorch = nn.TransformerEncoderLayer(d_model=768,\n",
        "                                             nhead=12,\n",
        "                                             dim_feedforward=3072,\n",
        "                                             dropout=0.1,\n",
        "                                             activation=\"gelu\",\n",
        "                                             batch_first=True,\n",
        "                                             norm_first=True)\n",
        "\n",
        "print(encoder_pytorch)\n",
        "print(encoder_pytorch(patch_and_position_embedding_with_tokens).shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ec0gurA5QLv",
        "outputId": "50c671b8-cc94-4c1e-a91e-440fd56e87e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TransformerEncoderLayer(\n",
            "  (self_attn): MultiheadAttention(\n",
            "    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
            "  )\n",
            "  (linear1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "  (dropout): Dropout(p=0.1, inplace=False)\n",
            "  (linear2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "  (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "  (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "  (dropout1): Dropout(p=0.1, inplace=False)\n",
            "  (dropout2): Dropout(p=0.1, inplace=False)\n",
            ")\n",
            "torch.Size([1, 197, 768])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "summary(model=encoder_pytorch,\n",
        "        input_size=(1, 197, 768),\n",
        "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
        "        col_width=20,\n",
        "        row_settings=[\"var_names\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BszQQ_727nKZ",
        "outputId": "20538fc0-67c3-4f9f-8e53-c2ba3e8ec027"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "==================================================================================================================================\n",
              "Layer (type (var_name))                            Input Shape          Output Shape         Param #              Trainable\n",
              "==================================================================================================================================\n",
              "TransformerEncoderLayer (TransformerEncoderLayer)  [1, 197, 768]        [1, 197, 768]        7,087,872            True\n",
              "==================================================================================================================================\n",
              "Total params: 7,087,872\n",
              "Trainable params: 7,087,872\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (M): 0\n",
              "==================================================================================================================================\n",
              "Input size (MB): 0.61\n",
              "Forward/backward pass size (MB): 0.00\n",
              "Params size (MB): 0.00\n",
              "Estimated Total Size (MB): 0.61\n",
              "=================================================================================================================================="
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Create a class called `ViT` that inherits from `torch.nn.Module`.\n",
        "2. Initialize the class with hyperparameters from Table 1 and Table 3 of the ViT paper for the ViT-Base model.\n",
        "3. Make sure the image size is divisible by the patch size (the image should be split into even patches).\n",
        "4. Calculate the number of patches using the formula $N = HW/P^2$, where $H$ is the image height, $W$ is the image width and $P$ is the patch size.\n",
        "5. Create a learnable class embedding token (equation 1).\n",
        "6. Create a learnable position embedding vector (equation 1).\n",
        "7. Setup the embedding dropout layer. \n",
        "  * *Dropout when used, is applied after every dense layer except for the qkv-projections and directly after adding positional-to-patch embeddings*.\n",
        "8. Create the patch embedding layer using the `PatchEmbedding` class.\n",
        "9. Create a series of Transformer Encoder blocks by passing a list of `TransformerEncoderBlock`s to `torch.nn.Sequential()` (equations 2 & 3).\n",
        "10. Create the MLP head (i.e. classifier head or equation 4) by passing a `torch.nn.LayerNorm()` (LN) layer and a `torch.nn.Linear(out_features=num_classes)` layer (where `num_classes` is the target number of classes) linear layer to `torch.nn.Sequential()`.\n",
        "11. Create a `forward()` method that accepts an input.\n",
        "12. Get the batch size of the input (the first dimension of the shape).\n",
        "13. Create the patching embedding using the layer created in step 8 (equation 1).\n",
        "14. Create the class token embedding using the layer created in step 5 and expand it across the number of batches found in step 11 using `torch.Tensor.expand()` (equation 1).\n",
        "15. Concatenate the class token embedding create in step 13 to the first dimension of the patch embedding created in step 12 using `torch.cat` (equation 1)."
      ],
      "metadata": {
        "id": "nCXktzVXw6DJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ViT(nn.Module):\n",
        "\n",
        "  def __init__(self, \n",
        "               height: int=224,\n",
        "               width: int=224,\n",
        "               color_channels: int=3,\n",
        "               patch_size: int=16,\n",
        "               batch_size: int=1,\n",
        "               num_layers: int=12,\n",
        "               embedding_dim: int=768,\n",
        "               mlp_size: int=3072,\n",
        "               num_heads: int=12,\n",
        "               dropout: float=0.1,\n",
        "               num_classes: int=1000):\n",
        "    super().__init__()\n",
        "\n",
        "    self.num_patches = int((height * width) / patch_size**2)\n",
        "    self.class_embedding = nn.Parameter(data=torch.randn(batch_size, 1, embedding_dim), \n",
        "                                     requires_grad=True)\n",
        "    self.position_embedding = nn.Parameter(data=torch.randn(batch_size, self.num_patches + 1, embedding_dim),\n",
        "                                           requires_grad=True)\n",
        "    self.dropout = nn.Dropout(p=dropout,\n",
        "                              inplace=True)\n",
        "    \n",
        "    self.embedded_patches = PatchEmbedding(in_channels=color_channels,\n",
        "                                           patch_size=patch_size,\n",
        "                                           embedding_dim=embedding_dim)\n",
        "\n",
        "    self.transformer_encoder = nn.Sequential(*[TransformerEncoder(embedding_dim=embedding_dim,\n",
        "                                                                  mlp_size=mlp_size,\n",
        "                                                                  num_heads=num_heads,\n",
        "                                                                  attn_dropout=0.0,\n",
        "                                                                  mlp_dropout=dropout) for _ in range(num_layers)])\n",
        "    \n",
        "    self.mlp_head = nn.Sequential(\n",
        "        nn.LayerNorm(normalized_shape=embedding_dim),\n",
        "        nn.Linear(in_features=embedding_dim, \n",
        "                  out_features=num_classes)\n",
        "    )\n",
        "\n",
        "  def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "    batch_size = x.shape[0]\n",
        "\n",
        "    xp = self.embedded_patches(x)\n",
        "\n",
        "    xp = torch.cat((self.class_embedding, xp), dim=1)\n",
        "    \n",
        "    xp += self.position_embedding\n",
        "\n",
        "    xp = self.dropout(xp)\n",
        "\n",
        "    out = self.transformer_encoder(xp)\n",
        "    \n",
        "    return self.mlp_head(out[:, 0, :])\n",
        "\n"
      ],
      "metadata": {
        "id": "k8br_zSHsPMF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = ViT()"
      ],
      "metadata": {
        "id": "ptQ0iskLHF5C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "out = model(image.unsqueeze(0))"
      ],
      "metadata": {
        "id": "9IJfkAeaI_c_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "out, out.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JqnoXYV4LEIi",
        "outputId": "ff53c99c-1a68-4e42-df25-27c7bb1c8605"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[0.1506]], grad_fn=<AddmmBackward0>), torch.Size([1, 1]))"
            ]
          },
          "metadata": {},
          "execution_count": 113
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup Optimizer\n",
        "\n",
        "\n",
        "Train all models, including ResNets, using Adam with $\\beta_{1}=0.9, \\beta_{2}=0.999$, a batch size of 4096 and apply a high weight decay of 0.1, which was found to be useful for trasfer of all models."
      ],
      "metadata": {
        "id": "Yn1OnOCT7MYl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.Adam(params=model.parameters(), \n",
        "                             lr=0.003,\n",
        "                             betas=(0.9, 0.999),\n",
        "                             weight_decay=0.1)\n",
        "optimizer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f7eFtZcw6SKB",
        "outputId": "31f8a340-4c90-4951-b6d0-256960715e39"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Adam (\n",
              "Parameter Group 0\n",
              "    amsgrad: False\n",
              "    betas: (0.9, 0.999)\n",
              "    capturable: False\n",
              "    differentiable: False\n",
              "    eps: 1e-08\n",
              "    foreach: None\n",
              "    fused: False\n",
              "    lr: 0.003\n",
              "    maximize: False\n",
              "    weight_decay: 0.1\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 119
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup Loss\n",
        "\n",
        "Searching the ViT paper for \"loss\" or \"loss function\" or \"criterion\" returns no results.\n",
        "\n",
        "Since the target problem is multi-class classification, we'll use `torch.nn.CrossEntropyLoss()`."
      ],
      "metadata": {
        "id": "-YA5dbfL-Ehr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn = nn.CrossEntropyLoss()\n",
        "loss_fn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oouLYBfc-Gn6",
        "outputId": "cf3b8361-c5f4-4589-985b-7e89a9eb46c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CrossEntropyLoss()"
            ]
          },
          "metadata": {},
          "execution_count": 121
        }
      ]
    }
  ]
}